{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c947354",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time, scipy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import wasserstein_distance\n",
        "import scipy.signal\n",
        "import time\n",
        "import freeDeconvolution\n",
        "#import subordination, sampling, elkaroui\n",
        "import multiprocessing as mp\n",
        "import json\n",
        "\n",
        "T = np.arange(0,10, 0.2)\n",
        "nu = np.array( [complex(0.1*a,b) for a in range(0  , 10) for b in [0.01,0.1]] )\n",
        "\n",
        "# Returns Scenario in the form of p eigenvalues\n",
        "def gen_matrix(Scenario, p):\n",
        "        np.random.seed(seed=None)\n",
        "        if Scenario==\"Case1\":\n",
        "            population_spectrum = np.ones( p )\n",
        "            weights = np.array( [1], dtype=np.float64 )\n",
        "            support = np.array( [1], dtype=np.float64 )\n",
        "\n",
        "        elif Scenario==\"Case2\" or Scenario==\"Case2.2\" or Scenario==\"Case2.3\" :\n",
        "            if Scenario==\"Case2\":\n",
        "                weights = np.array( [1, 1], dtype=np.float64 )\n",
        "                support = np.array( [1, 2], dtype=np.float64 )\n",
        "            elif Scenario==\"Case2.2\" :\n",
        "                weights = np.array( [1, 1], dtype=np.float64 )\n",
        "                support = np.array( [1, 1.3], dtype=np.float64 )\n",
        "            else :\n",
        "                weights = np.array( [1, 1, 1, 1, 1], dtype=np.float64 )\n",
        "                support = np.array( [1, 2, 3, 5, 6], dtype=np.float64 )\n",
        "            weights = weights/np.sum( weights )\n",
        " \n",
        "            population_cdf = np.cumsum( weights )\n",
        "\n",
        "            population_spectrum = np.zeros( (p,) )\n",
        "            block_begin = 0\n",
        "            for i in range( len(weights) ):\n",
        "                block_end = int( population_cdf[i]*p )\n",
        "                population_spectrum[block_begin:block_end] = support[i]\n",
        "                block_begin = block_end\n",
        "\n",
        "        elif Scenario==\"Case3\":\n",
        "            c = 1\n",
        "            indices = np.arange( 0, p, 1)\n",
        "            toeplitz_row    = 0.3**indices\n",
        "            toeplitz = scipy.linalg.toeplitz( toeplitz_row)\n",
        "            \n",
        "            population_spectrum, U = np.linalg.eig(toeplitz)\n",
        "            population_spectrum = np.sort( population_spectrum )\n",
        "\n",
        "            weights = np.ones(p)*1.0/p\n",
        "            support = population_spectrum\n",
        "\n",
        "        else:\n",
        "            print( \"Please specify a scenario...\" )\n",
        "            raise Error()\n",
        "        \n",
        "        return (population_spectrum, weights, support)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e3754a40",
      "metadata": {},
      "source": [
        "# I. Generate experiments JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87352fd7",
      "metadata": {},
      "outputs": [],
      "source": [
        "N_range   = [ 1024 ]\n",
        "nb_itr    = 1\n",
        "#Scenarios = [ \"Case1\", \"Case2\", \"Case2.2\", \"Case2.3\", \"Case3\"]\n",
        "Scenarios = [ \"Case2.3\"]\n",
        "\n",
        "DOEs = []\n",
        "\n",
        "# Loop over scenarios\n",
        "for Scenario in Scenarios:\n",
        "    print( f'''  |- Scenario {Scenario}''')\n",
        "    # Loop over iterations\n",
        "    for iteration in range(nb_itr):\n",
        "        # Loop over N\n",
        "        for N in N_range:\n",
        "            T = np.arange(0,10, 0.2)\n",
        "            nu = np.array( [complex(0.1*a,b) for a in range(0  , 10) for b in [0.01,0.1]] )\n",
        "            c = 1\n",
        "            p = int(c*N)\n",
        "\n",
        "            generated_data = gen_matrix(Scenario, p)\n",
        "            population_spectrum, weights, support = generated_data\n",
        "            diag           = freeDeconvolution.sampling.sample_wishart( p, N, population_spectrum )\n",
        "\n",
        "            population_cdf = np.zeros_like( T )\n",
        "            for i in range( len(T) ):\n",
        "                t = T[i]\n",
        "                population_cdf[i] = np.count_nonzero( population_spectrum <= t )\n",
        "            population_cdf = population_cdf/p\n",
        "            #\n",
        "            DOEs.append( {\n",
        "                \"Scenario\": Scenario,\n",
        "                \"N\"       : N,\n",
        "                \"observed_spec\"    : list(diag),\n",
        "                \"population_spec\"  : list(population_spectrum),\n",
        "                \"population_cdf\"   : list(population_cdf),\n",
        "                \"truth_weights\"    : list(weights),\n",
        "                \"truth_support\"    : list(support),\n",
        "                \"results_by_method\": {}\n",
        "            } )\n",
        "        #\n",
        "    #\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30700c90",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"./DOEs.json\", \"w\") as f:\n",
        "    json.dump( DOEs, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4efc12c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# WARNING: Crushes old results\n",
        "with open(\"./DOEs_with_results.json\", \"w\") as f:\n",
        "    json.dump( DOEs, f, indent=4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8d2a5fd8",
      "metadata": {},
      "source": [
        "# II. Loading and processing DOEs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c5b53133",
      "metadata": {},
      "source": [
        "## II.1. El Karoui's method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "254bf816",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"./DOEs_with_results.json\", \"r\") as f:\n",
        "    DOEs = json.load(f)\n",
        "#\n",
        "print( f'''Loaded {len(DOEs)} experiments...''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4024b46e",
      "metadata": {},
      "outputs": [],
      "source": [
        "methods = [ \"convex_optim\", \"subordination\", \"our method\"]\n",
        "method  = methods[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f7c49be",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_DOE_with_convex( DOE, norm=\"l2\"):\n",
        "    c = 1\n",
        "    T = np.arange(0,10, 0.2)\n",
        "    nu = np.array( [complex(0.1*a,b) for a in range(0  , 10) for b in [0.01,0.1]] )\n",
        "\n",
        "    # RMT data\n",
        "    diag = np.array( DOE[\"observed_spec\"] )\n",
        "    population_spectrum = np.array( DOE[\"population_spec\"] )\n",
        "    population_cdf      = np.array( DOE[\"population_cdf\"] )\n",
        "\n",
        "    ## El Karoui\n",
        "    tic = time.time()\n",
        "    Z = freeDeconvolution.elkaroui.build_dictionary( nu, c, diag)\n",
        "    nu_check = -(1-c)/Z + c*freeDeconvolution.elkaroui.stieltjes(Z, diag)\n",
        "    nu_errors = np.abs(nu - nu_check)\n",
        "    bad_indices = np.where(nu_errors > 1e-5)\n",
        "\n",
        "    # Clean-up if necessary\n",
        "    if len(bad_indices):\n",
        "        Z  = np.delete( Z , bad_indices )\n",
        "        nu = np.delete( nu, bad_indices )\n",
        "    dictionary = (Z, nu)\n",
        "\n",
        "    # Perform optimization\n",
        "    weights_convex, objective_value = freeDeconvolution.elkaroui.perform_cvx_optimization( dictionary, T, c, norm, verbose=False)\n",
        "    weights_convex = abs(weights_convex)\n",
        "    toc    = time.time()\n",
        "    timing = toc-tic\n",
        "    print( f'''Timing for convex optimization by el Karoui {str(timing)}''' )\n",
        "\n",
        "    ## Done\n",
        "    error   = wasserstein_distance( T, population_spectrum,  weights_convex, np.ones(len(population_spectrum))/len(population_spectrum))\n",
        "    new_DOE = DOE.copy()\n",
        "    new_DOE[\"results_by_method\"][\"convex_optim\"] = {\n",
        "        \"error\": error,\n",
        "        \"timing\": timing,\n",
        "        \"weights\": list(weights_convex),\n",
        "        \"support\": list(T)\n",
        "    }\n",
        "\n",
        "    return new_DOE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfd60d22",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loop over experiments\n",
        "print( \"Loop over DOEs using multiprocessing... \")\n",
        "num_processes = 12 # Use all the available CPU cores of computer\n",
        "with mp.Pool(processes=num_processes) as pool:\n",
        "    results = pool.map( compute_DOE_with_convex , DOEs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "513c49dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"./DOEs_with_results.json\", \"w\") as f:\n",
        "    json.dump( results, f, indent=4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b67c6b32",
      "metadata": {},
      "source": [
        "## II.2. Tarrago's method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdb1d1ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"./DOEs_with_results.json\", \"r\") as f:\n",
        "    DOEs = json.load(f)\n",
        "#\n",
        "print( f'''Loaded {len(DOEs)} experiments...''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c3661bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "## II.2. Processing with the subordination method\n",
        "methods = [ \"convex_optim\", \"subordination\"]\n",
        "method  = methods[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "662d1c9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_DOE_with_subordination( DOE):\n",
        "    # RMT data\n",
        "    diag = np.array( DOE[\"observed_spec\"] )\n",
        "    population_spectrum = np.array( DOE[\"population_spec\"] )\n",
        "    population_cdf      = np.array( DOE[\"population_cdf\"] )\n",
        "    \n",
        "    ## Tarrago\n",
        "    tic  = time.time()\n",
        "    y, R = freeDeconvolution.subordination.freedeconvolutionresult(diag)\n",
        "    weights_subordination = abs(R)/np.sum(R)\n",
        "    toc    = time.time()\n",
        "    timing = toc-tic\n",
        "    print( f'''Timing for convex optimization by Tarrago {str(timing)}''' )\n",
        "\n",
        "    ## Done\n",
        "    error   = wasserstein_distance( y, population_spectrum,  weights_subordination, np.ones(len(population_spectrum))/len(population_spectrum))\n",
        "    new_DOE = DOE.copy()\n",
        "    new_DOE[\"results_by_method\"][\"subordination\"] = {\n",
        "        \"error\"  : error,\n",
        "        \"timing\" : timing,\n",
        "        \"weights\": list(weights_subordination),\n",
        "        \"support\": list(y)\n",
        "    }\n",
        "\n",
        "    return new_DOE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d398d30d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loop over experiments\n",
        "print( \"Loop over DOEs using multiprocessing... \")\n",
        "num_processes = 12 # Use all the available CPU cores of computer\n",
        "with mp.Pool(processes=num_processes) as pool:\n",
        "    results = pool.map( compute_DOE_with_subordination , DOEs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd86e9dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"./DOEs_with_results.json\", \"w\") as f:\n",
        "    json.dump( results, f, indent=4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c10253cc",
      "metadata": {},
      "source": [
        "## II.3. Our method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4497a727",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"./DOEs_with_results.json\", \"r\") as f:\n",
        "    DOEs = json.load(f)\n",
        "#\n",
        "print( f'''Loaded {len(DOEs)} experiments...''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e573e4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "## II.3. Processing with our method\n",
        "methods = [ \"convex_optim\", \"subordination\", \"our method\"]\n",
        "method  = methods[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aa7cb33",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_DOE_with_our_method( DOE, debug=True, debug_aggressive=False, plot=True):\n",
        "    c = 1\n",
        "    N = DOE['N']\n",
        "    p = int(c*N)\n",
        "        \n",
        "    # RMT data\n",
        "    diag = np.array( DOE[\"observed_spec\"] )\n",
        "    population_spectrum = np.array( DOE[\"population_spec\"] )\n",
        "    population_cdf      = np.array( DOE[\"population_cdf\"] )\n",
        "    \n",
        "    tic  = time.time()\n",
        "\n",
        "    ## Init\n",
        "    mu_observed = freeDeconvolution.core.DiscreteMeasure( diag, None)\n",
        "    mu_signal   = freeDeconvolution.core.DiscreteMeasure( population_spectrum, None)\n",
        "\n",
        "    mu_observed.compute_second_kind()\n",
        "\n",
        "    zeroes_first_kind   = mu_observed.zeroes_first_kind\n",
        "    zeroes_second_kind  = mu_observed.zeroes_second_kind\n",
        "\n",
        "    ## Find bounding box\n",
        "    from freeDeconvolution import boxes\n",
        "    degree = len(diag)\n",
        "\n",
        "    if debug:\n",
        "        print(\"\")\n",
        "        print(\"-----------------------------------\")\n",
        "        print(\"id       : \", DOE['id'])\n",
        "        print(\"Scenario : \", DOE['Scenario'])\n",
        "        print(\"\")\n",
        "\n",
        "    mesh_size = 10000\n",
        "    # radius = np.max(diag)/2 + 1\n",
        "    # center = np.max(diag)/2\n",
        "    # interval = np.linspace(0, 2*np.pi, mesh_size)\n",
        "    # contour = center + radius*( np.cos(interval) + np.sin(interval)*1.0j)\n",
        "    # plt.scatter( np.real(diag), np.imag(diag), c='r')\n",
        "    # plt.plot( np.real(contour), np.imag(contour) )\n",
        "    # plt.show()\n",
        "\n",
        "    def index_integrand(z):\n",
        "        values = mu_observed.Markov_Krein_prime(z)/mu_observed.Markov_Krein(z)\n",
        "        return values\n",
        "\n",
        "    # values = index_integrand(contour)\n",
        "    # dz = 1.0j*(contour-center)*2*np.pi/(mesh_size) \n",
        "    # index  = np.sum(dz*values)/(2*np.pi*1.0j)\n",
        "    # print( \"Index: \", index)\n",
        "    # print( \"Root count: \", index+2*degree)\n",
        "\n",
        "    # print( \"Box segments enumeration: \")\n",
        "    # print( boxes.box_segments_enum )\n",
        "\n",
        "    def compute_index( box, mesh_size, plot=True, color='b'):\n",
        "        interval =  np.linspace( 0,1, mesh_size)\n",
        "        integral = 0\n",
        "        for segment in boxes.box_segments_enum:\n",
        "                vector = box[ segment[1] ] - box[ segment[0] ]\n",
        "                origin = box[ segment[0] ]\n",
        "                s = origin + interval*vector\n",
        "                #\n",
        "                values = index_integrand(s)\n",
        "                dz = ( s[-1]-s[0] )/mesh_size\n",
        "                integral = integral + np.sum( values*dz )\n",
        "        return integral/(2*np.pi*1.0j)\n",
        "    # TODO: Make it more versatile. Here tuning by hand of radius.\n",
        "\n",
        "    radius = 4\n",
        "    mesh_size = int(1e4)\n",
        "    bounding_box = {\n",
        "        'top_left'    : np.min(diag) - 0.3 + radius*1.0j,\n",
        "        'bottom_right': np.max(diag) + 0.3 - radius*1.0j,\n",
        "    }\n",
        "    bounding_box   = boxes.extend_box(bounding_box)\n",
        "    index = compute_index( bounding_box, mesh_size)\n",
        "    index = np.real(index+2*degree)\n",
        "    root_count = int( np.round( index ) )\n",
        "    error = index-root_count\n",
        "    if debug:\n",
        "        print( \"Checking the number of critical points in initial bounding box\")\n",
        "        print( \"Index of contour : \", index)\n",
        "        print( \"Root count: \", \"2x\", 0.5*root_count)\n",
        "        print( \"p         : \", p)\n",
        "        print( \"\")\n",
        "\n",
        "    # If false, the bounding box missed roots\n",
        "    assert( p-1 == int(0.5*root_count))\n",
        "    \n",
        "    ## Find small contour\n",
        "    find_small_contour = True\n",
        "    if not find_small_contour:\n",
        "        if debug:\n",
        "            print(\"Using default bounding box...\")\n",
        "        contour_height = radius\n",
        "    else:\n",
        "        if debug:\n",
        "            print(\"Finding near-optimal bounding box...\")\n",
        "        #\n",
        "        box = bounding_box.copy()\n",
        "        box['bottom_left'] = box['top_left'] # For initialization, bottom_left needs to be the previous top_left\n",
        "        radius = box['height']/2\n",
        "        stop_at_first_nonempty = True        # Stop at first found box with roots\n",
        "\n",
        "        # Loop for multiple passes and more\n",
        "        boxes_with_roots = []\n",
        "        boxes_with_roots.append( box )\n",
        "        root_counter = 0\n",
        "        total_roots  = p-1 # Total number of roots in upper half plane\n",
        "        i = 0\n",
        "        while( root_counter < total_roots ):\n",
        "            i = i+1\n",
        "            radius = radius/2\n",
        "            box = {\n",
        "                    'top_left'    : box['bottom_left'],\n",
        "                    'bottom_right': np.real( box['bottom_right'] ) + radius*1.0j,\n",
        "            }\n",
        "            box   = boxes.extend_box(box)\n",
        "            index = compute_index( box, mesh_size, plot=False)\n",
        "            index = np.real(index)\n",
        "            root_count = int( np.round( index ) )\n",
        "            root_counter = root_counter + root_count\n",
        "            error = index-root_count\n",
        "            if debug_aggressive:\n",
        "                print(f\"Pass {i}:\")\n",
        "                print( \"Index: \", index)\n",
        "                print( \"Root count / Total: \", root_count, '/', root_counter)\n",
        "                print( \"Found:\", root_counter, \" / \", total_roots )\n",
        "                print( \"\")\n",
        "            #\n",
        "            if root_count>0:\n",
        "                    box['root_count'] = root_count\n",
        "                    boxes_with_roots.append( box )\n",
        "                    if stop_at_first_nonempty:\n",
        "                            break\n",
        "        # end  while\n",
        "        contour_height = 2*radius\n",
        "    # end if find_small_contour\n",
        "\n",
        "    ## Setup arrays\n",
        "    contour_type = \"rectangle\"\n",
        "    mesh_size    = int(1e4)\n",
        "    left_point   = np.min(diag)\n",
        "    right_point  = np.max(diag)\n",
        "    mid_point    = 0.5*left_point + 0.5*right_point\n",
        "    \n",
        "    # New bounding box\n",
        "    print( \"Contour height: \", contour_height )\n",
        "    box = {\n",
        "            'top_left'    : np.real( bounding_box['top_left'],    ) - 1.0 + contour_height*1.0j,\n",
        "            'bottom_right': np.real( bounding_box['bottom_right'] ) + 1.0 - contour_height*1.0j,\n",
        "    }\n",
        "    box   = boxes.extend_box(box)\n",
        "    print( \"Bounding box height (new): \", box )\n",
        "    bounding_box = box\n",
        "\n",
        "\n",
        "    if contour_type==\"rectangle\":\n",
        "        path = boxes.box_to_path( bounding_box, mesh_size )\n",
        "        z_array = np.array( path )\n",
        "    elif contour_type=='circle':\n",
        "        radius = np.maximum( 1.1*(right_point-left_point)/2 , bounding_box[\"height\"] )\n",
        "        #\n",
        "        interval = np.linspace(0, 2*np.pi, mesh_size)\n",
        "        z_array = mid_point + radius*( np.cos(interval) + np.sin(interval)*1.0j)\n",
        "    \n",
        "    m_array = mu_observed.M_empirical( z_array )\n",
        "    s_array = (1+m_array)/(m_array*z_array)\n",
        "\n",
        "    # Various arrays\n",
        "    s_signal_array = s_array\n",
        "    s_noise_array  = 1/(c*m_array + 1)\n",
        "    s_deconv_array = s_signal_array/s_noise_array\n",
        "    #\n",
        "    # NEXT LINE IS F*** UP\n",
        "    # m_deconv_array = 1/( s_deconv_array*z_array - 1)\n",
        "    #\n",
        "    # TENTATIVE FIX\n",
        "    m_deconv_inv_array = (1+1/m_array)/s_deconv_array\n",
        "    m_deconv_array   = m_array\n",
        "    z_array_original = z_array\n",
        "    z_array          = m_deconv_inv_array\n",
        "    #\n",
        "    #m_deconv_theoretical_array = mu_signal.M_empirical( z_array )\n",
        "    g_deconv_array = (m_deconv_array+1)/z_array\n",
        "    #g_deconv_theoretical = (m_deconv_theoretical_array+1)/z_array\n",
        "\n",
        "\n",
        "    ## Plot of newly found contour\n",
        "    if plot:\n",
        "        plt.figure()\n",
        "        plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
        "        plt.plot( np.real(z_array), np.imag(z_array), label=\"Transformed z contour\" )\n",
        "        plt.plot( np.real(z_array_original), np.imag(z_array_original), label=\"Original z contour\" )\n",
        "        plt.scatter( zeroes_first_kind, np.zeros_like(zeroes_first_kind), label=\"Roots of $mu_n$\")\n",
        "        plt.title( f'''Contour for DOE id={DOE['id']}''' )\n",
        "        plt.legend()\n",
        "        plt.savefig(f'''contour_id_{DOE['id']}.png''' )\n",
        "\n",
        "    ## Compute moments\n",
        "    dz_array  = z_array-np.roll(z_array, shift=1)\n",
        "    # Rectangle Riemann integration\n",
        "    def cauchy_integral_g_deconv( f ):\n",
        "        value = g_deconv_array*f*dz_array\n",
        "        #value = g_deconv_theoretical*f*dz_array\n",
        "        return value.sum()/(2*np.pi*1.0j)\n",
        "    # Trapezoid Riemann integration\n",
        "    def cauchy_integral_trapezoid_g_deconv( f ):\n",
        "        integrand = g_deconv_array*f\n",
        "        value = 0.5*( integrand + np.roll(integrand, shift=1) )*dz_array\n",
        "        return value.sum()/(2*np.pi*1.0j)\n",
        "    moments_count = 5\n",
        "    mom_array       = np.zeros( 2*moments_count + 2)\n",
        "    mom_array_truth = np.zeros( 2*moments_count + 2)\n",
        "    truth_weights   = np.array( DOE['truth_weights'] )\n",
        "    truth_support   = np.array( DOE['truth_support'] )\n",
        "    centering       = 0\n",
        "    for mom_index in range( len(mom_array) ):\n",
        "        value = cauchy_integral_trapezoid_g_deconv( (z_array-centering)**mom_index ) # Centering stabilizes\n",
        "        mom_array[ mom_index ] = np.real(value)\n",
        "        # Activate centering\n",
        "        # if mom_index==1:\n",
        "        #      centering    = mom_array[1]\n",
        "        #      mom_array[1] = 0\n",
        "        mom_array_truth[mom_index] = np.dot( truth_weights, (truth_support-centering)**mom_index)\n",
        "    # end for\n",
        "\n",
        "    # Chebyshev\n",
        "    endpoint_left  = np.min( np.real(z_array) )\n",
        "    endpoint_right = np.max( np.real(z_array) )\n",
        "    centering = 0.5*( endpoint_left + endpoint_right )\n",
        "    scale     = endpoint_right - centering\n",
        "    normalized_z_array = (z_array-centering)/scale\n",
        "    \n",
        "    chebyshev_z_array     = compute_chebyshev_values( normalized_z_array, 2*moments_count + 2)\n",
        "    chebyshev_truth_array = compute_chebyshev_values( (truth_support-centering)/scale, 2*moments_count + 2)\n",
        "    chebyshev_truth_array = np.real( chebyshev_truth_array )\n",
        "\n",
        "    #print( \"Chebyshev on support :\", chebyshev_truth_array)\n",
        "\n",
        "    chebyshev_mom_array       = np.zeros( 2*moments_count + 2)\n",
        "    chebyshev_mom_array_truth = np.zeros( 2*moments_count + 2)\n",
        "    for mom_index in range( len(mom_array) ):\n",
        "        value = cauchy_integral_trapezoid_g_deconv( chebyshev_z_array[mom_index] ) # Centering stabilizes\n",
        "        chebyshev_mom_array[ mom_index ] = np.real(value)\n",
        "        chebyshev_mom_array_truth[mom_index] = np.dot( truth_weights, chebyshev_truth_array[mom_index] )\n",
        "    # end for\n",
        "\n",
        "    if debug:\n",
        "        np.set_printoptions(precision=5, suppress=True)\n",
        "        #print( \"Midpoint: \", mid_point )\n",
        "        print( \"Estimated moments: \")\n",
        "        print( mom_array )\n",
        "        print( \"Ground truth:\")\n",
        "        print( mom_array_truth )\n",
        "        print( \"Absolute error:\")\n",
        "        print( np.abs(mom_array-mom_array_truth) )\n",
        "        print( \"Relative error:\")\n",
        "        print( np.abs( (mom_array-mom_array_truth)*100/mom_array_truth ) )\n",
        "        print( \"\")\n",
        "        print( \"Estimated Chebyshev moments: \")\n",
        "        print( chebyshev_mom_array )\n",
        "        print( \"Ground truth:\")\n",
        "        print( chebyshev_mom_array_truth )\n",
        "        print( \"Absolute error:\")\n",
        "        print( np.abs( chebyshev_mom_array-chebyshev_mom_array_truth ) )\n",
        "        print( \"Relative error:\")\n",
        "        print( np.abs( (chebyshev_mom_array-chebyshev_mom_array_truth)*100/chebyshev_mom_array_truth ) )\n",
        "        print( \"\")\n",
        "\n",
        "    # WARNING: For debug\n",
        "    chebyshev_mom_array = chebyshev_mom_array_truth\n",
        "\n",
        "    ## Inverse moment problem\n",
        "    print( \"Performing inverse Chebychev moment problem...\")\n",
        "    jacobi_a, jacobi_b = jacobi_from_chebyshev_moments( chebyshev_mom_array )\n",
        "    print( \"a :\", jacobi_a )\n",
        "    print( \"b :\", jacobi_b )\n",
        "    support, weights   = freeDeconvolution.quadrature_from_jacobi( jacobi_a, jacobi_b)\n",
        "    print( \"Centered support: \", support )\n",
        "    print( \"Weights: \", weights)\n",
        "    support = scale*support + centering\n",
        "    non_outliers = np.where( (support>left_point)*(support<right_point) )[0]\n",
        "    support = support[non_outliers]\n",
        "    weights = weights[non_outliers]\n",
        "    weights = weights/np.sum(weights)\n",
        "    error   = wasserstein_distance( support, population_spectrum, weights, np.ones(len(population_spectrum))/len(population_spectrum))\n",
        "    \n",
        "    print( \"Error  : \", error)\n",
        "    print( \"Weights: \", weights)\n",
        "    print( \"Support: \", support)\n",
        "    print( \"\" )\n",
        "\n",
        "    ## Inverse moment problem\n",
        "    print( \"Performing inverse moment problem...\")\n",
        "    jacobi_a, jacobi_b = freeDeconvolution.oprl.jacobi_from_moments( mom_array )\n",
        "    print( \"a :\", jacobi_a )\n",
        "    print( \"b :\", jacobi_b )\n",
        "    support, weights   = freeDeconvolution.quadrature_from_jacobi( jacobi_a, jacobi_b)\n",
        "    support = support # + centering\n",
        "    non_outliers = np.where( (support>left_point)*(support<right_point) )[0]\n",
        "    support = support[non_outliers]\n",
        "    weights = weights[non_outliers]\n",
        "    weights = weights/np.sum(weights)\n",
        "    error   = wasserstein_distance( support, population_spectrum, weights, np.ones(len(population_spectrum))/len(population_spectrum))\n",
        "    \n",
        "    print( \"Error  : \", error)\n",
        "    print( \"Weights: \", weights)\n",
        "    print( \"Support: \", support)\n",
        "    print( \"\" )\n",
        "\n",
        "    # Ground truth\n",
        "    print( \"Ground truth: \")\n",
        "    print( truth_weights )\n",
        "    print( truth_support )\n",
        "    print( \"\")\n",
        "\n",
        "    # Timings\n",
        "    toc    = time.time()\n",
        "    timing = toc-tic\n",
        "    print( f'''Timing for our method {str(timing)}''' )\n",
        "    \n",
        "    ## Recording\n",
        "    new_DOE = DOE.copy()\n",
        "    new_DOE[\"results_by_method\"][\"our method\"] = {\n",
        "        \"error\"  : error,\n",
        "        \"timing\" : timing,\n",
        "        \"weights\": list(weights),\n",
        "        \"support\": list(support)\n",
        "    }\n",
        "\n",
        "    with open( f'''./dump/DOE_{DOE['id']}.json''', \"w\") as f:\n",
        "        json.dump( new_DOE, f, indent=4)\n",
        "\n",
        "    return new_DOE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53a28404",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_chebyshev_values( x, order ):\n",
        "    result = []\n",
        "    #\n",
        "    term_0 = np.ones_like(x)*1.0\n",
        "    result.append( term_0 )\n",
        "    term_1 = x\n",
        "    result.append( x )\n",
        "    for i in range(2, order+1):\n",
        "            # Compute next term in recurrence\n",
        "            # Normally P_{n+1} = 2X P_n - P_{n-1}\n",
        "            # But P_n = 2^{n-1} T_n\n",
        "            # Hence T_{n+1} = X P_n - (1/4)*P_{n-1}\n",
        "            if i==2:\n",
        "                 b2 = 0.5\n",
        "            else:\n",
        "                 b2 = 0.25\n",
        "            term_2 = x*term_1 - b2*term_0\n",
        "            result.append( term_2 )\n",
        "            # Shift variables\n",
        "            term_0 = term_1 \n",
        "            term_1 = term_2\n",
        "    # end for\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03de3382",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Input: Array of 2n+1 moments from c_0=1 to c_{2n+1}\n",
        "def jacobi_from_chebyshev_moments( mom_array, debug=False ):\n",
        "    assert( (len(mom_array)-1) % 2 == 1 )\n",
        "    mom_array = np.hstack( (mom_array, [0]))\n",
        "    \n",
        "    # Form Gram matrix\n",
        "    # Uses the fact that\n",
        "    # 2 P_n P_m = P_{n+m} + P_{|n-m|} for the (non-monic) Chebyshev\n",
        "    # This leads to\n",
        "    # T_n T_m = T_{n+m} + 2^{|m-n|-n-m} T_{|n-m|}\n",
        "    #         = T_{n+m} + 2^{-2*min(n,m)} T_{|n-m|}\n",
        "    # Except when n=m where\n",
        "    # T_n^2 = T_{2n} + 2^{-2n+1}\n",
        "    n = int( 0.5*(len(mom_array)-2) )\n",
        "    gram_matrix = np.zeros( shape=(n+2, n+2) )\n",
        "    for i in range( n+2 ):\n",
        "        if i==0:\n",
        "            for j in range( i+1, n+2):\n",
        "                gram_matrix[i,j] = mom_array[i+j]\n",
        "        else:\n",
        "            for j in range( i+1, n+2):\n",
        "                gram_matrix[i,j] = mom_array[i+j] + (2**(-2*i))*mom_array[j-i]\n",
        "    gram_matrix = gram_matrix + gram_matrix.transpose()\n",
        "    for index in range(1, n+2 ):\n",
        "        gram_matrix[index, index] = mom_array[2*index] + 2**(-2*index+1)\n",
        "    gram_matrix[0,0] = 1\n",
        "\n",
        "    if debug:\n",
        "        print( \"Gram matrix: \", gram_matrix)\n",
        "    \n",
        "    # Cholesky\n",
        "    # try:\n",
        "    #     cholesky = scipy_cholesky( mom_matrix, lower=True )\n",
        "    #     print(\"Cholesky passed!\")\n",
        "    #     print(\"\")\n",
        "    # except np.linalg.LinAlgError as err:\n",
        "    #     print( err  )\n",
        "    #     print(\"\")\n",
        "\n",
        "    # Use of LAPACK wrapper in scipy\n",
        "    # https://stackoverflow.com/questions/49101574/scipy-numpy-cholesky-while-checking-if-positive-definite\n",
        "    (cholesky, minor) = scipy.linalg.lapack.dpotrf( gram_matrix, lower=True  )\n",
        "    if minor > 0:\n",
        "        # if debug:\n",
        "        #     print( f'''{minor}-th principal minor is not positive definite''')\n",
        "        print( f'''{minor}-th principal minor is not positive definite''')\n",
        "        gram_matrix = gram_matrix[:minor, :minor]\n",
        "        cholesky   = cholesky[:minor, :minor]\n",
        "        #\n",
        "        n = minor - 1\n",
        "\n",
        "    if debug:\n",
        "        print( \"Cholesky matrix: \")\n",
        "        print( cholesky )\n",
        "\n",
        "    mom_count = cholesky.shape[0]\n",
        "    diag_indices, extra_indices = freeDeconvolution.oprl.jacobi_indices( mom_count )\n",
        "    diag_cholesky = cholesky.T[diag_indices[0,:] , diag_indices[1,:]]\n",
        "    extra_diag    = cholesky.T[extra_indices[0,:], extra_indices[1,:]]\n",
        "\n",
        "    if debug:\n",
        "        print( \"Diagonal of Cholesky\")\n",
        "        print( diag_cholesky )\n",
        "        print( \"Extra-diagonal of Cholesky\")\n",
        "        print( extra_diag )\n",
        "        print(\"\")\n",
        "\n",
        "    # Compute Jacobi\n",
        "    jacobi_b = diag_cholesky[1:]/diag_cholesky[:-1]\n",
        "    jacobi_b = jacobi_b[:-1]\n",
        "    jacobi_a = np.zeros_like( extra_diag )\n",
        "    if len(extra_diag)>0:\n",
        "        jacobi_a[0] = extra_diag[0]\n",
        "        x_over_y = extra_diag/diag_cholesky[:-1]\n",
        "        jacobi_a[1:] = x_over_y[1:]-x_over_y[:-1]\n",
        "\n",
        "    return jacobi_a, jacobi_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ff0aafe",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import string\n",
        "\n",
        "# Tag DOEs for identification if bugs\n",
        "for i in range( len(DOEs) ):\n",
        "    DOE       = DOEs[i]\n",
        "    DOE['id'] = 31000+i\n",
        "\n",
        "# Lazy mode\n",
        "lazy = False\n",
        "if lazy:\n",
        "    print( \"Working in lazy mode...\")\n",
        "    files = os.listdir('./dump')\n",
        "    str_identifiers = [ f.strip(string.ascii_letters)[1:-1] for f in files]\n",
        "    identifiers     = sorted( [ int(str_id)-31000 for str_id in str_identifiers] )\n",
        "    print( \"Found files: \", identifiers )\n",
        "    all_indices      = set( range(len(DOEs)) )\n",
        "    selected_indices = all_indices.difference( identifiers )\n",
        "    selected_indices = list(selected_indices)[:]\n",
        "    print( f'''Number of selected indices: {len(selected_indices)}/{len(DOEs)}''')\n",
        "    print( \"Selected indices:\", selected_indices)\n",
        "    selected_DOEs = [ DOEs[i] for i in selected_indices ]\n",
        "    print( \"\")\n",
        "else:\n",
        "    selected_DOEs = DOEs\n",
        "\n",
        "# Loop over experiments\n",
        "print( \"Loop over DOEs using multiprocessing... \")\n",
        "num_processes = 1 # Use all the available CPU cores of computer\n",
        "with mp.Pool(processes=num_processes) as pool:\n",
        "    results = pool.map( compute_DOE_with_our_method, selected_DOEs)\n",
        "# for DOE in selected_DOEs:\n",
        "#     compute_DOE_with_our_method( DOE )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc1704ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"./DOEs_with_results.json\", \"w\") as f:\n",
        "    json.dump( results, f, indent=4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "75795e13",
      "metadata": {},
      "source": [
        "## II.4. Fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c6cc8e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import string\n",
        "\n",
        "files = os.listdir('./dump')\n",
        "DOEs_fused = []\n",
        "for filename in files:\n",
        "    f = open( f'''./dump/{filename}''', 'r')\n",
        "    DOE = json.load( f )\n",
        "    DOEs_fused.append( DOE )\n",
        "# end for\n",
        "\n",
        "with open(\"./DOEs_fused.json\", \"w\") as f:\n",
        "    json.dump( DOEs_fused, f, indent=4)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "acbf1736",
      "metadata": {},
      "source": [
        "# III. Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c7a3095",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"./DOEs_with_results.json\", \"r\") as f:\n",
        "    DOEs = json.load(f)\n",
        "#\n",
        "print( f'''Loaded {len(DOEs)} experiments...''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0271d9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "methods = [ \"convex_optim\", \"subordination\", \"our method\"]\n",
        "\n",
        "aggregated_results = freeDeconvolution.plots.aggregate_benchmarks( DOEs, methods)\n",
        "freeDeconvolution.plots.make_plots( aggregated_results, methods )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4eaf711c",
      "metadata": {},
      "source": [
        "# IV. Analysis\n",
        "\n",
        "Here we dump data to csv file for further analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afa52d58",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"./DOEs_with_results.json\", \"r\") as f:\n",
        "    DOEs = json.load(f)\n",
        "#\n",
        "print( f'''Loaded {len(DOEs)} experiments...''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a41bb829",
      "metadata": {},
      "outputs": [],
      "source": [
        "extracts = []\n",
        "for DOE in DOEs:\n",
        "    result   = DOE['results_by_method']['our method']\n",
        "    error    = result['error']\n",
        "    timing   = result['timing']\n",
        "    extract = [ DOE['Scenario'], DOE['N'], error, timing, result[\"weights\"], result[\"support\"] ]\n",
        "    extracts.append( extract )\n",
        "# end for\n",
        "\n",
        "import csv\n",
        "\n",
        "col_names = [\"Scenario\", \"N\", \"error\", \"timing\", \"weights\", \"support\"]\n",
        "\n",
        "with open(\"./extracts.csv\", \"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow( col_names )\n",
        "    writer.writerows(extracts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dab49aec",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "6RybRximYwwI"
      ],
      "name": "Shared_ElKaroui3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
